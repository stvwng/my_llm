{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads a LLM with OpenAI's GPT2 weights and then fine-tunes the model with a dataset of instructions and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import torch\n",
    "import instruction_dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import gpt_model\n",
    "import pretrain_model\n",
    "import load_pretrained_weights\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load instruction dataset\n",
    "with open(\"instruction-data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Number of entries: \", len(data))\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training, test, and validation sets\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length: \", len(train_data))\n",
    "print(\"Test set length: \", len(test_data))\n",
    "print(\"Validation set length: \", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available:\n",
    "    device = torch.device(\"mps\") # Apple Silicon\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "collate_function = partial(\n",
    "    instruction_dataset.collate,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n",
    "\n",
    "train_dataset = instruction_dataset.InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn = collate_function,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataset = instruction_dataset.InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_function,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_dataset = instruction_dataset.InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_function,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "'''\n",
    "print(\"Train loader\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    print(input_batch.shape, target_batch.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI GPT2 settings and parameters\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings: \", settings)\n",
    "print()\n",
    "print(\"Params keys: \", params.keys())\n",
    "print(\"Params token embedding weights: \", params[\"wte\"])\n",
    "print(\"Token embedding weights shape: \", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize our LLM and load OpenAI GPT2 weights\n",
    "\n",
    "gpt = gpt_model.GPTModel(\n",
    "    vocab_size = 50257,\n",
    "    context_length = 1024,\n",
    "    emb_dim = 768,\n",
    "    num_heads = 12,\n",
    "    num_layers = 12,\n",
    "    drop_rate = 0.0,\n",
    "    qkv_bias = True\n",
    ")\n",
    "load_pretrained_weights.load_weights_into_gpt(gpt, params)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate initial loss for training and validation sets\n",
    "gpt.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = pretrain_model.calc_loss_loader(data_loader=train_loader, model=gpt, device=device, num_batches=5)\n",
    "    val_loss = pretrain_model.calc_loss_loader(data_loader=val_loader, model=gpt, device=device, num_batches=5)\n",
    "\n",
    "print(\"Training loss: \", train_loss)\n",
    "print(\"Validation loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune LLM \n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = pretrain_model.pretrain_model_simple(\n",
    "    model=gpt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=instruction_dataset.format_input(val_data[0]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]: # first 3 test set samples\n",
    "    input_text = instruction_dataset.format_input(entry)\n",
    "    token_ids = pretrain_model.generate(\n",
    "        model=gpt,\n",
    "        index=pretrain_model.text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=1024,\n",
    "        eos_id=50256\n",
    "    )\n",
    "    \n",
    "    generated_text = pretrain_model.token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    \n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel reponse:\\n>> {response_text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = instruction_dataset.format_input(entry)\n",
    "    token_ids = pretrain_model.generate(\n",
    "        model=gpt,\n",
    "        index=pretrain_model.text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=1024,\n",
    "        eos_id=50256\n",
    "    )\n",
    "    \n",
    "    generated_text = pretrain_model.token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "    \n",
    "    with open(\"intruction-data-with-response.json\", \"w\") as file:\n",
    "        json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_response(index):\n",
    "    print(f\"Instruction: {test_data[index]['instruction']}\")\n",
    "    print()\n",
    "    print(f\"Input: {test_data[index]['input']}\")\n",
    "    print()\n",
    "    print(f\"Output: {test_data[index]['output']}\")\n",
    "    print()\n",
    "    print(f\"Model response: {test_data[index]['model_response']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_response(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_response(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
